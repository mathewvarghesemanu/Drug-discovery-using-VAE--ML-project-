{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ddJefvdz30p"
      },
      "source": [
        "# <center> ML Lab 2 Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXaYoWKPzx6P"
      },
      "source": [
        "### <center> Mathew Varghese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIktryJizrpr"
      },
      "source": [
        "# Reading data and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5G1XR7KMc7R"
      },
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from rdkit import Chem\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 526,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmYBzPORyLlN",
        "outputId": "5f2f3069-aaf1-421c-b629-e1ff450e8e88"
      },
      "source": [
        "# Accessing google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#loading files\n",
        "df = pd.read_csv('/content/drive/My Drive/plakshadrug/AA.csv')\n",
        "df=df.iloc[:35000,:]\n",
        "smiles = df['smiles'].tolist()\n",
        "df.shape"
      ],
      "execution_count": 527,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 527
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1DdtwvF0CPi"
      },
      "source": [
        "# Preparing data for the VAE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfAnxUE5yy6m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845af128-9abe-4d85-b4bf-af45e4b3996e"
      },
      "source": [
        "# charset for smiles data\n",
        "\n",
        "charset = set(\"\".join(list(df.smiles))+\"!E\")\n",
        "\n",
        "# padding smiles to 120 characters\n",
        "char_to_int = dict( (c,i) for i,c in enumerate( charset ) )\n",
        "def char_encoder( characters, maxlen=120 ):\n",
        "    characters = Chem.MolToSmiles(Chem.MolFromSmiles( characters ))\n",
        "    padded_char = np.zeros( ( maxlen, len( charset ) ) )\n",
        "    for i, c in enumerate( characters ):\n",
        "        padded_char[i, char_to_int[c] ] = 1\n",
        "    return padded_char\n",
        "\n",
        "# decoding smiles\n",
        "int_to_char = dict( (i,c) for i,c in enumerate( charset ) )\n",
        "def char_decoder( X ):\n",
        "    X = X.argmax( axis=-1 )\n",
        "    decoded_char = ''\n",
        "    for i in X:\n",
        "        decoded_char += int_to_char[ i ]\n",
        "    return decoded_char\n",
        "# onehot encoding \n",
        "one_hot_encoded_data = np.array([char_encoder(x) for x in smiles])\n",
        "one_hot_encoded_data.shape\n"
      ],
      "execution_count": 528,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 120, 52)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 528
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6KTAfN7k6vg"
      },
      "source": [
        "# adding a dimension\n",
        "one_hot_encoded_data = one_hot_encoded_data.reshape(35000,120,52,1)\n",
        "# preparing data for training\n",
        "#batch size is 32\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(one_hot_encoded_data).batch(32))"
      ],
      "execution_count": 529,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJ1I658u0N1O"
      },
      "source": [
        "# Define VAE, encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOAZq5LAk6yq"
      },
      "source": [
        "#define VAE as a class\n",
        "class VAE(tf.keras.Model):\n",
        "\n",
        "\n",
        "  def __init__(self, latent_dim):\n",
        "    super(VAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    # defining encoder\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(120, 52, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "        ]\n",
        "    )\n",
        "    # defining decoder\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=30*13*32, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(30, 13, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(120, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  def encode(self, x):\n",
        "    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits"
      ],
      "execution_count": 530,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a72IDw4Hk61R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e809e03c-777e-4b55-9ace-343ec4748701"
      },
      "source": [
        "# Define loss fn and training step\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "  x_logit = tf.cast(x_logit, tf.float64)\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  logpz = tf.cast(logpz, tf.float64)\n",
        "  logqz_x = tf.cast(logqz_x, tf.float64)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "# train steps\n",
        "model = VAE(latent_dim)\n",
        "epochs = 100\n",
        "print(\"VAE started!!\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "  for train_x in train_dataset:\n",
        "    train_step(model, train_x, optimizer)\n",
        "  print(\"epoch:\", epoch, \" of \", epochs, \" total epochs \")\n",
        " "
      ],
      "execution_count": 531,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VAE started!!\n",
            "epoch: 1  of  100  total epochs \n",
            "epoch: 2  of  100  total epochs \n",
            "epoch: 3  of  100  total epochs \n",
            "epoch: 4  of  100  total epochs \n",
            "epoch: 5  of  100  total epochs \n",
            "epoch: 6  of  100  total epochs \n",
            "epoch: 7  of  100  total epochs \n",
            "epoch: 8  of  100  total epochs \n",
            "epoch: 9  of  100  total epochs \n",
            "epoch: 10  of  100  total epochs \n",
            "epoch: 11  of  100  total epochs \n",
            "epoch: 12  of  100  total epochs \n",
            "epoch: 13  of  100  total epochs \n",
            "epoch: 14  of  100  total epochs \n",
            "epoch: 15  of  100  total epochs \n",
            "epoch: 16  of  100  total epochs \n",
            "epoch: 17  of  100  total epochs \n",
            "epoch: 18  of  100  total epochs \n",
            "epoch: 19  of  100  total epochs \n",
            "epoch: 20  of  100  total epochs \n",
            "epoch: 21  of  100  total epochs \n",
            "epoch: 22  of  100  total epochs \n",
            "epoch: 23  of  100  total epochs \n",
            "epoch: 24  of  100  total epochs \n",
            "epoch: 25  of  100  total epochs \n",
            "epoch: 26  of  100  total epochs \n",
            "epoch: 27  of  100  total epochs \n",
            "epoch: 28  of  100  total epochs \n",
            "epoch: 29  of  100  total epochs \n",
            "epoch: 30  of  100  total epochs \n",
            "epoch: 31  of  100  total epochs \n",
            "epoch: 32  of  100  total epochs \n",
            "epoch: 33  of  100  total epochs \n",
            "epoch: 34  of  100  total epochs \n",
            "epoch: 35  of  100  total epochs \n",
            "epoch: 36  of  100  total epochs \n",
            "epoch: 37  of  100  total epochs \n",
            "epoch: 38  of  100  total epochs \n",
            "epoch: 39  of  100  total epochs \n",
            "epoch: 40  of  100  total epochs \n",
            "epoch: 41  of  100  total epochs \n",
            "epoch: 42  of  100  total epochs \n",
            "epoch: 43  of  100  total epochs \n",
            "epoch: 44  of  100  total epochs \n",
            "epoch: 45  of  100  total epochs \n",
            "epoch: 46  of  100  total epochs \n",
            "epoch: 47  of  100  total epochs \n",
            "epoch: 48  of  100  total epochs \n",
            "epoch: 49  of  100  total epochs \n",
            "epoch: 50  of  100  total epochs \n",
            "epoch: 51  of  100  total epochs \n",
            "epoch: 52  of  100  total epochs \n",
            "epoch: 53  of  100  total epochs \n",
            "epoch: 54  of  100  total epochs \n",
            "epoch: 55  of  100  total epochs \n",
            "epoch: 56  of  100  total epochs \n",
            "epoch: 57  of  100  total epochs \n",
            "epoch: 58  of  100  total epochs \n",
            "epoch: 59  of  100  total epochs \n",
            "epoch: 60  of  100  total epochs \n",
            "epoch: 61  of  100  total epochs \n",
            "epoch: 62  of  100  total epochs \n",
            "epoch: 63  of  100  total epochs \n",
            "epoch: 64  of  100  total epochs \n",
            "epoch: 65  of  100  total epochs \n",
            "epoch: 66  of  100  total epochs \n",
            "epoch: 67  of  100  total epochs \n",
            "epoch: 68  of  100  total epochs \n",
            "epoch: 69  of  100  total epochs \n",
            "epoch: 70  of  100  total epochs \n",
            "epoch: 71  of  100  total epochs \n",
            "epoch: 72  of  100  total epochs \n",
            "epoch: 73  of  100  total epochs \n",
            "epoch: 74  of  100  total epochs \n",
            "epoch: 75  of  100  total epochs \n",
            "epoch: 76  of  100  total epochs \n",
            "epoch: 77  of  100  total epochs \n",
            "epoch: 78  of  100  total epochs \n",
            "epoch: 79  of  100  total epochs \n",
            "epoch: 80  of  100  total epochs \n",
            "epoch: 81  of  100  total epochs \n",
            "epoch: 82  of  100  total epochs \n",
            "epoch: 83  of  100  total epochs \n",
            "epoch: 84  of  100  total epochs \n",
            "epoch: 85  of  100  total epochs \n",
            "epoch: 86  of  100  total epochs \n",
            "epoch: 87  of  100  total epochs \n",
            "epoch: 88  of  100  total epochs \n",
            "epoch: 89  of  100  total epochs \n",
            "epoch: 90  of  100  total epochs \n",
            "epoch: 91  of  100  total epochs \n",
            "epoch: 92  of  100  total epochs \n",
            "epoch: 93  of  100  total epochs \n",
            "epoch: 94  of  100  total epochs \n",
            "epoch: 95  of  100  total epochs \n",
            "epoch: 96  of  100  total epochs \n",
            "epoch: 97  of  100  total epochs \n",
            "epoch: 98  of  100  total epochs \n",
            "epoch: 99  of  100  total epochs \n",
            "epoch: 100  of  100  total epochs \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Dq7J7sB0X5s"
      },
      "source": [
        "# Decode molecules from latent space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dQtW9rk6_G"
      },
      "source": [
        "# makes molecules decoded from the latent space.\n",
        "def latent_space_matrix(model, lat_vars):\n",
        "  \n",
        "  \n",
        "  norm = tfp.distributions.Normal(0, 1)\n",
        "  grid_x = norm.quantile(np.linspace(0.05, 0.95, 10))\n",
        "  grid_y = norm.quantile(np.linspace(0.05, 0.95, 10))\n",
        "#  specifying width and height of matrix\n",
        "  image = np.zeros((52, 120))\n",
        "\n",
        "  for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "      a = lat_vars[0] if lat_vars[0] != 0 else xi\n",
        "      b = lat_vars[1] if lat_vars[1] != 0 else yi\n",
        "      c = np.array([[a, b]]) \n",
        "      decoded = model.sample(c)\n",
        "      drug = tf.reshape(decoded[0], (120, 52))\n",
        "      return drug\n",
        "\n",
        "#convert latent space variable to one hot encoded\n",
        "def convert_latent(vec_float):\n",
        "  max_arg = np.argmax(vec_float)\n",
        "  mol_vec = [1 if j==max_arg else 0 for j,_ in enumerate(vec_float)]\n",
        "  return mol_vec"
      ],
      "execution_count": 532,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzGtvO0Y0dUP"
      },
      "source": [
        "# Printing new molecules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4--uw1ofk7Ku",
        "outputId": "6b61b795-82f7-4ac5-9f67-5ad0ff49182f"
      },
      "source": [
        "# Convert and print new molecules in smile format with 120 characters\r\n",
        "\r\n",
        "lspace=[1,3,6,9,16]\r\n",
        "for i in range (0,5):\r\n",
        "  mol = latent_space_matrix(model,lat_vars = [lspace[i],0])\r\n",
        "  mol_decode = [convert_latent(i) for i in mol]\r\n",
        "  new_drug=char_decoder(np.array(mol_decode))\r\n",
        "  print(\"Drug \",i+1 ,\" :  \",new_drug)\r\n"
      ],
      "execution_count": 534,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drug  1  :   C=C[C((((C)CC==NNNCNNOOOO//gg#M#g77n66##n#####77))))3#]ZCC]a2+1K6#R(.9(B6c)2..o95-2C\\rB6oB@P6-(P)l@K1-=(rVcn)IeerB)KII)e\n",
            "Drug  2  :   C[C@@](O))CCCCNNNO))NNONN)eet#t77777M777R#####7)M)2(@#][[K9H259PXXoX.9(]/CCC((PR]-@/33g=o9@P6-M]9lgMM-=(PPMM2577MBagF.)X\n",
            "Drug  3  :   C[C@@]FOO)CCCCCNO)))))/NN)eet#t7887MM(77R8###)77MM2@@c#tgK#ns[tteXoXP#(//CZZZ#t@@@//33g=o(tFFXX]9]@@M3@@PcMK2P77M%aaFF)X\n",
            "Drug  4  :   C[C@@rPOO)CC@@C(()))))/N))eO##t788(MM(77ttt#C)C7MM2@@cttgK#ns[PtSSooP33//CZZt#t@@]//#37//(tttXM99l@@3333tcMB2P@/M##gF77X\n",
            "Drug  5  :   C[C@@rPOO)CC@CC(())))/tt))eOO-t78898O(OOttt#C)CHH]2O@cttgK#ns[PtSSooP33/tCZ3t#t@-]//##t//(ttttM99\\ttM-@@tcBttP@/t###F77t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}